{
  "gen.no_segments": "No segments.jsonl found. Run cleaning first.",
  "gen.no_valid_segments": "No valid text segments found.",
  "gen.resume_found": "üîÑ Found {skip} existing samples, resuming from segment {next}...",
  "gen.starting": "Generating dataset with [{model}]...",
  "gen.connecting": "üì° Connecting to Ollama...\n   Model: {model}\n   Mode: {mode}\n   Segments: {total}\n   Skipping completed: {skip}",
  "gen.test_hello": "Hello",
  "gen.test_reply": "Reply OK",
  "gen.connect_ok": "‚úÖ Ollama connected\n   Model response: {response}\n   Done reason: {reason}",
  "gen.connect_fail": "‚ùå Ollama connection failed: {error}",
  "gen.connect_error": "Cannot connect to Ollama: {error}",
  "gen.segment_header": "\n‚îÄ‚îÄ Segment {current}/{total} ‚îÄ‚îÄ\nüìÑ Text: {preview}...",
  "gen.empty_response": "‚ùå Empty AI response\n   Response fields: {fields}\n   done_reason: {reason}",
  "gen.progress_status": "Generated {success} samples ({failed} failed)",
  "gen.ai_response": "ü§ñ AI response ({length} chars): {preview}",
  "gen.style_rejected": "‚ö†Ô∏è Style quality check: output too similar to source ({similarity}), skipped",
  "gen.lang_mismatch": "‚ö†Ô∏è Language mismatch: source script={src}, output script={out}, skipped",
  "gen.lang_mismatch_keep_small": "‚ö†Ô∏è Language mismatch: source script={src}, output script={out}; kept because this is a tiny batch to avoid total failure",
  "gen.progress_style": "Generated {success} samples ({failed} failed, {rejected} too similar)",
  "gen.success": "‚úÖ Success! Total {count} samples\n   Q: {preview}...",
  "gen.json_mismatch": "‚ö†Ô∏è JSON field mismatch: {keys}",
  "gen.json_parse_fail": "‚ùå JSON parse failed\n   AI raw: {text}",
  "gen.network_error": "‚ùå Network error: {error}",
  "gen.exception": "‚ùå Exception: {type}: {error}",
  "gen.summary": "\n‚ïê‚ïê Generation Complete ‚ïê‚ïê\n   ‚úÖ Success: {success}\n   ‚ùå Failed: {failed}\n   üìä Total: {total}",
  "gen.no_valid_data": "No valid data generated ({total} segments all failed). Check AI logs for details.",
  "gen.saved": "üíæ Saved: train.jsonl ({train} samples), valid.jsonl ({valid} samples)",

  "gen.prompt.qa.system": "You are a professional training data generation expert. Your task is to generate a high-quality Q&A pair based on the given text.\nRequirements:\n1. Questions should be in-depth, not simple fact extraction ‚Äî test understanding and analysis\n2. Vary question types: comprehension, analysis, reasoning, application\n3. Answers should be complete, well-organized, with sufficient detail and explanation\n4. Answers should be based on the text but organized in your own words, not directly copied\n5. Output JSON directly: {\"question\": \"...\", \"answer\": \"...\"}",
  "gen.prompt.qa.user": "Based on the following text, generate an in-depth Q&A pair. The question should test understanding and analysis. The answer should be complete and well-organized. Output JSON only.\n\n[Text]\n{text}",
  "gen.prompt.style.system": "You are a writing style analysis and imitation expert. Your task is:\n1. Analyze the writing style of the given sample (vocabulary, sentence structure, rhetoric, narrative perspective, emotional tone, rhythm, etc.)\n2. Create a \"writing instruction\" and \"stylized response\":\n   - instruction: a concrete writing task grounded in the source topic/characters/setting\n   - output: a rewritten/expanded/continued passage that stays aligned with the source topic while preserving style\n\nCritical rules:\n- output MUST stay aligned with the source topic/characters/setting; do not switch to unrelated themes\n- output can rewrite, expand, or continue the source, but must not mechanically copy it\n- output's writing style (vocabulary, sentence structure, rhetoric, tone) must be highly consistent with the original\n- instruction should be concise and task-focused with no extra explanation\n- output language must remain the same as the input text (no translation)\nOutput JSON directly: {\"instruction\": \"...\", \"output\": \"...\"}",
  "gen.prompt.style.user": "Carefully analyze the writing style of the following sample (vocabulary, sentence structure, rhetoric, tone, rhythm, etc.), then create a writing instruction and corresponding stylized response.\nNote: output must stay aligned with the source topic and may rewrite/expand/continue it, but should not copy the source. Output language must match the source language. Output JSON only.\n\n[Writing Sample]\n{text}",
  "gen.prompt.chat.system": "You are a professional dialogue data generation expert. Your task is to convert the given text into a natural, in-depth multi-turn dialogue (at least 3 turns).\nRequirements:\n1. Dialogue should be natural and fluent, like a real teacher-student Q&A or friend discussion\n2. User questions should progress from basic to in-depth\n3. Assistant responses should be professional, detailed, guiding deeper discussion\n4. Include follow-up questions, clarifications, examples, and other natural dialogue elements\n5. Don't simply split the text into dialogue ‚Äî discuss the topic naturally\nOutput JSON directly: {\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}",
  "gen.prompt.chat.user": "Convert the following text into a natural multi-turn dialogue (at least 3 rounds). The dialogue should progress naturally with follow-ups and deeper discussion. Output JSON only.\n\n[Text]\n{text}",
  "gen.prompt.instruct.system": "You are a professional instruction data generation expert. Your task is to generate a high-quality instruction-output pair based on the given text.\nRequirements:\n1. Vary instruction types: summarize, analyze, compare, reason, explain, rewrite, expand, evaluate, etc.\n2. Instructions should be clear and specific\n3. Output should be high-quality, well-organized, demonstrating good understanding and expression\n4. Output should not directly copy the original text, but reorganize based on understanding\nOutput JSON directly: {\"instruction\": \"...\", \"output\": \"...\"}",
  "gen.prompt.instruct.user": "Based on the following text, generate a high-quality instruction-output pair. Choose instruction type from: summarize key points, in-depth analysis, comparison, causal reasoning, concept explanation, opinion evaluation. Output JSON only.\n\n[Text]\n{text}",
  "gen.prompt.keep_language": "Output language must stay the same as the input text. Do not translate or switch language.",

  "clean.raw_not_found": "Raw directory not found: {path}",
  "clean.no_files": "No files found in raw directory",
  "clean.starting": "Starting cleaning... ({count} files)",
  "clean.processing": "Processing: {filename}",
  "clean.cleaned": "‚úÖ Cleaned {filename} ‚Üí {segments} segments",
  "clean.skip_empty": "‚ö†Ô∏è Skipped (empty): {filename}",
  "clean.error_file": "‚ùå Error processing {filename}: {error}",
  "clean.complete": "Cleaning complete: {segments} segments from {files} files",
  "clean.docx_not_installed": "python-docx not installed, skipping .docx: {filename}. Install with: pip install python-docx",
  "clean.pdf_not_installed": "PyPDF2 not installed, skipping .pdf: {filename}. Install with: pip install PyPDF2",

  "export.ollama_not_found": "Ollama is not installed or not running.",
  "export.model_not_found": "Cannot resolve model: {model}",
  "export.adapter_not_found": "Adapter not found: {path}",
  "export.no_adapter_weights": "No adapter weights in: {path}",
  "export.fuse_start": "Fusing adapter with base model...\n  Model: {model}\n  Adapter: {adapter}",
  "export.gguf_try": "Trying GGUF export (Llama/Mistral only)...",
  "export.gguf_ok": "GGUF exported: {filename}",
  "export.gguf_fallback": "GGUF not supported for this model. Using MLX API dequantize...",
  "export.fuse_fail": "Fuse + dequantize failed:\n{error}",
  "export.verify_start": "Verifying safetensors compatibility (F32/F16/BF16 only)...",
  "export.verify_done": "Verified: {details}",
  "export.verify_warn": "Verify warning: {error} (will try anyway)",
  "export.model_ready": "Model ready ({format}): {filename}",
  "export.creating": "Creating Ollama model '{name}' from {format} (quantize: {quant})...",
  "export.running_cmd": "Running: {cmd}",
  "export.retry_no_quant": "Retrying without --quantize...",
  "export.create_fail": "Ollama create failed:\n{error}",
  "export.create_fail_unknown": "Ollama create failed",
  "export.loading_mlx": "Loading model + adapter via MLX API...",
  "export.fusing_lora": "Fusing LoRA layers...",
  "export.fused_count": "Fused {count} LoRA layers",
  "export.dequantizing": "Dequantizing all quantized weights...",
  "export.added_arch": "Added architectures: [{arch}]",
  "export.saving": "Saving dequantized model...",
  "export.removed_tensors": "removed {count} incompatible tensors",
  "export.tensors_ready": "{count} tensors ready",
  "export.config_cleaned": "cleaned config",

  "inference.loading": "Loading model...",
  "inference.generating": "Generating...",
  "inference.model_not_found": "Model directory not found: {path}",
  "inference.config_not_found": "Model config.json not found at: {path}. This may not be a valid MLX model directory.",
  "inference.adapter_not_found": "Adapter directory not found: {path}",
  "inference.not_cached": "Model {model} not in local cache, mlx_lm will attempt to download...",

  "download.not_installed": "huggingface_hub not installed. Run: pip install huggingface_hub",
  "download.not_found": "Model not found: {repo}",
  "download.gated": "Gated model requires authentication: {repo}",
  "download.info_fail": "Failed to get model info: {error}",

  "builtin.no_segments": "No segments.jsonl found. Run cleaning first.",
  "builtin.no_valid_segments": "No valid segments found.",
  "builtin.starting": "Generating built-in dataset ({count} segments, mode: {mode})...",
  "builtin.complete": "Built-in generation complete: {count} samples",
  "builtin.saved": "üíæ Saved: train.jsonl ({train} samples), valid.jsonl ({valid} samples)",
  "builtin.heading_question": "Please introduce {heading}",
  "builtin.explain": "Explain: {topic}",
  "builtin.what_is": "What is {topic}?",
  "builtin.describe": "Describe {topic}.",
  "builtin.tell_me": "Tell me about {topic}.",
  "builtin.style_1": "Write a descriptive passage in the same writing style.",
  "builtin.style_2": "Create new content maintaining the same writing style.",
  "builtin.style_3": "Continue writing in the same literary style and tone.",
  "builtin.style_4": "Compose a new paragraph using the same narrative voice.",
  "builtin.style_5": "Write a passage mimicking this author's style and rhetoric.",
  "builtin.instruct_1": "Summarize the key points of the following:",
  "builtin.instruct_2": "Briefly describe the following text:",
  "builtin.instruct_3": "Analyze the main ideas in the following:",
  "builtin.instruct_4": "Interpret the following content:",
  "builtin.instruct_5": "Explain the deeper meaning of the following:",
  "builtin.instruct_6": "Analyze the following from different perspectives:"
}
