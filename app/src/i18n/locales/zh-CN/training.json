{
  "title": "训练",
  "pageTitle": "2. 训练模型",
  "selectProject": "选择一个项目来配置训练",
  "noProjects": "暂无项目。请先创建一个项目。",
  "params": {
    "title": "训练参数",
    "model": "基础模型",
    "modelHint": "HuggingFace 仓库 ID 或本地路径（如 mlx-community/Llama-3.2-3B-Instruct-4bit）",
    "iters": "训练迭代次数",
    "itersHint": "训练迭代的总次数。更多次数 = 训练更久但效果可能更好。",
    "batchSize": "批次大小",
    "batchSizeHint": "每批次的样本数。如果显存不够请降低此值。",
    "loraLayers": "LoRA 层数",
    "loraLayersHint": "应用 LoRA 的层数。更多层 = 更多可训练参数。",
    "loraRank": "LoRA 秩",
    "loraRankHint": "LoRA 矩阵的秩。更高的秩 = 更强的表达能力但占用更多内存。",
    "learningRate": "学习率",
    "learningRateHint": "优化步长。太高 → 不稳定，太低 → 收敛慢。",
    "seed": "随机种子",
    "seedHint": "设为 0 表示随机。固定种子可复现结果。",
    "loraScale": "LoRA 缩放",
    "loraScaleHint": "调整幅度大小。值越大，模型的改变越明显。",
    "loraDropout": "LoRA Dropout",
    "loraDropoutHint": "随机丢弃比例。适当增大可防止死记硬背（过拟合）。",
    "maxSeqLength": "最大文本长度",
    "maxSeqLengthHint": "每条训练数据的最大长度。根据你数据的平均长度选择。",
    "gradCheckpoint": "梯度检查点",
    "gradCheckpointHint": "开启可节省内存但训练变慢。内存不够时再开。",
    "stepsPerEval": "验证间隔",
    "stepsPerEvalHint": "每训练多少轮检查一次效果。",
    "stepsPerReport": "报告间隔",
    "stepsPerReportHint": "每训练多少轮刷新一次界面数据。",
    "valBatches": "验证批次",
    "valBatchesHint": "验证时用多少批数据。越多越准但越慢。",
    "optimizer": "优化器",
    "optimizerHint": "权重更新算法。Adam 是最常用的默认选择。",
    "gradAccumulationSteps": "梯度累积步数",
    "gradAccumulationStepsHint": "累积 N 个批次后再更新权重。等效增大批次但不增加内存。",
    "saveEvery": "定期保存间隔",
    "saveEveryHint": "每训练 N 轮保存一次适配器检查点，防止进度丢失。",
    "maskPrompt": "屏蔽提示词损失",
    "maskPromptHint": "只计算回答部分的损失，忽略提示词部分。可提升 SFT 质量。",
    "showAdvanced": "展开高级参数",
    "hideAdvanced": "收起高级参数"
  },
  "method": {
    "title": "训练方法",
    "lora": "LoRA",
    "loraDesc": "低秩自适应。通过训练小型适配器矩阵高效微调，推荐大多数场景使用。",
    "dora": "DoRA",
    "doraDesc": "权重分解低秩自适应。增强版 LoRA，效果更好，计算量略增。",
    "full": "Full",
    "fullDesc": "全参数微调。训练所有模型权重，需要大量内存。",
    "hint": "请先选择模型和数据集，再选择训练方法。"
  },
  "presets": {
    "title": "快速预设",
    "quick": "快速测试（100 次迭代）",
    "standard": "标准训练（1000 次迭代）",
    "thorough": "深度训练（2000 次迭代）",
    "extreme": "极致训练（5000 次迭代）"
  },
  "start": "开始训练",
  "stop": "停止训练",
  "log": "训练日志",
  "noLog": "训练日志将在开始训练后显示。",
  "status": {
    "idle": "准备就绪",
    "running": "训练进行中...",
    "completed": "训练完成！",
    "failed": "训练失败"
  },
  "section": {
    "selectModel": "选择所需训练的模型",
    "selectDataset": "选择数据集",
    "method": "训练方法",
    "params": "训练参数"
  },
  "completedBanner": "训练已完成！模型适配器已保存。",
  "savedAt": "保存位置：",
  "goToTest": "去测试模型",
  "selectModelHint": "请选择一个模型开始训练",
  "localModelPath": "本地模型路径",
  "invalidModelPath": "该路径下未检测到有效的 MLX 模型文件",
  "hfModelHint": "HuggingFace 模型（训练时自动下载到 ~/.cache/huggingface/hub/）",
  "noDataset": "未检测到训练数据集，请先在「数据准备」页面生成",
  "selectDatasetVersion": "选择数据集版本",
  "datasetLegacy": "旧版数据集",
  "dataset": {
    "trainSet": "训练集",
    "validSet": "验证集",
    "samples": "{{count}} 条",
    "sourceFiles": "来源文件",
    "genType": "生成类型",
    "genMethod": "生成方式",
    "methodOllama": "AI 生成（{{model}}）",
    "methodBuiltin": "内置规则",
    "modeQa": "知识问答",
    "modeStyle": "风格模仿",
    "modeChat": "对话模式",
    "modeInstruct": "通用指令",
    "page": "第 {{current}} / {{total}} 页",
    "prevPage": "上一页",
    "nextPage": "下一页"
  },
  "trainProgress": "训练进度",
  "initializing": "初始化中...",
  "eta": {
    "title": "预计剩余时间",
    "estimating": "正在估算中...",
    "done": "已完成",
    "format": "约 {{time}}"
  },
  "health": {
    "title": "训练健康状态",
    "level": {
      "green": "正常",
      "yellow": "需关注",
      "red": "异常"
    },
    "hintNormal": "训练过程稳定，可继续等待。",
    "hintLossFluctuating": "Loss 有轻微波动，通常属于正常现象。",
    "hintLossRising": "Loss 呈上升趋势，建议检查数据质量或学习率。",
    "hintOutOfMemory": "检测到内存不足，建议降低 Batch 或开启梯度检查点。",
    "hintError": "检测到训练错误，请查看日志定位问题。"
  },
  "trend": {
    "title": "Loss 趋势",
    "good": "训练正在收敛，效果良好",
    "stable": "Loss 基本稳定，可继续观察",
    "warning": "Loss 波动较大，建议关注"
  },
  "alerts": {
    "title": "智能告警",
    "count": "{{count}} 条进行中",
    "none": "当前无活跃告警，训练状态稳定。",
    "memory": {
      "title": "检测到内存压力",
      "detail": "训练已触发内存上限，可能即将失败。",
      "action": "请降低 Batch 或最大序列长度后重新开始训练。"
    },
    "runtime": {
      "title": "检测到运行时异常",
      "detail": "最近日志中出现异常特征信息。",
      "action": "请先查看训练日志，修复脚本或模型相关问题。"
    },
    "thermal": {
      "title": "存在热降频风险",
      "detail": "设备温度可能正在限制训练速度。",
      "action": "建议关闭高负载应用，或短暂停止训练后继续。"
    },
    "stalled": {
      "title": "训练进度可能停滞",
      "detail": "一段时间内未观察到新的迭代进展。",
      "action": "请检查 I/O 与日志；若持续停滞，建议停止并用更稳参数重启。"
    },
    "lossRising": {
      "title": "Loss 呈上升趋势",
      "detail": "近期训练损失有明显上升。",
      "action": "请检查数据质量，必要时降低学习率。"
    }
  },
  "latestTrainLoss": "最新 Train Loss:",
  "lossCurve": "Loss 曲线",
  "waitingData": "等待训练数据...",
  "lossStopped": "训练已停止，暂无足够数据绘制曲线。",
  "lossNoData": "暂无足够数据绘制曲线。",
  "invalidModelError": "所选路径下未检测到有效模型，无法开始训练",
  "paramsSummary": "{{iters}} 次迭代 · Batch {{batch}} · {{method}}",
  "entries": "{{count}} 条",
  "step": {
    "model": "模型",
    "data": "数据",
    "method": "方法",
    "params": "参数",
    "done": "完成"
  },
  "summary": {
    "title": "训练总结",
    "duration": "训练时长",
    "finalTrainLoss": "最终训练损失",
    "finalValLoss": "最终验证损失",
    "totalIters": "总迭代次数",
    "baseModel": "基础模型",
    "adapterPath": "适配器保存位置",
    "keyParams": "关键参数",
    "lossImprove": "损失改善",
    "goToExport": "导出模型",
    "noData": "无数据",
    "report": {
      "copyMd": "复制报告 Markdown",
      "share": "系统分享"
    }
  },
  "validation": {
    "needModel": "请先选择训练模型",
    "needDataset": "请先选择数据集",
    "needMethod": "请先选择训练方法"
  }
}
