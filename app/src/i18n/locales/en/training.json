{
  "title": "Training",
  "pageTitle": "2. Train Model",
  "selectProject": "Select a project to configure training",
  "noProjects": "No projects available. Create a project first.",
  "params": {
    "title": "Training Parameters",
    "model": "Base Model",
    "modelHint": "HuggingFace repo ID or local path (e.g. mlx-community/Llama-3.2-3B-Instruct-4bit)",
    "iters": "Training Iterations",
    "itersHint": "Number of training iterations. More iterations = longer training but potentially better results.",
    "batchSize": "Batch Size",
    "batchSizeHint": "Samples per batch. Lower if you run out of memory.",
    "loraLayers": "LoRA Layers",
    "loraLayersHint": "Number of layers to apply LoRA. More layers = more parameters to train.",
    "loraRank": "LoRA Rank",
    "loraRankHint": "Rank of LoRA matrices. Higher rank = more expressive but more memory.",
    "learningRate": "Learning Rate",
    "learningRateHint": "Step size for optimization. Too high → unstable, too low → slow.",
    "seed": "Random Seed",
    "seedHint": "Set to 0 for random. Use a fixed seed for reproducible results.",
    "loraScale": "LoRA Scale",
    "loraScaleHint": "Adjustment magnitude. Higher values = more obvious changes to the model.",
    "loraDropout": "LoRA Dropout",
    "loraDropoutHint": "Random drop ratio. Increase slightly to prevent overfitting (memorizing data).",
    "maxSeqLength": "Max Sequence Length",
    "maxSeqLengthHint": "Max length per training sample. Choose based on your data's average length.",
    "gradCheckpoint": "Gradient Checkpoint",
    "gradCheckpointHint": "Saves memory but slows training. Enable when running low on memory.",
    "stepsPerEval": "Eval Interval",
    "stepsPerEvalHint": "Check model performance every N iterations.",
    "stepsPerReport": "Report Interval",
    "stepsPerReportHint": "Update training metrics on screen every N iterations.",
    "valBatches": "Validation Batches",
    "valBatchesHint": "How many batches to use for validation. More = more accurate but slower.",
    "optimizer": "Optimizer",
    "optimizerHint": "Algorithm for weight updates. Adam is the most common default.",
    "gradAccumulationSteps": "Gradient Accumulation",
    "gradAccumulationStepsHint": "Accumulate N batches before updating. Effectively multiplies batch size without extra memory.",
    "saveEvery": "Save Every N Iters",
    "saveEveryHint": "Save adapter checkpoint every N iterations. Protects against losing progress.",
    "maskPrompt": "Mask Prompt Loss",
    "maskPromptHint": "Only compute loss on completions, ignore the prompt part. Improves SFT quality.",
    "showAdvanced": "Show Advanced Parameters",
    "hideAdvanced": "Hide Advanced Parameters"
  },
  "method": {
    "title": "Training Method",
    "lora": "LoRA",
    "loraDesc": "Low-Rank Adaptation. Efficient fine-tuning by training small adapter matrices. Recommended for most use cases.",
    "dora": "DoRA",
    "doraDesc": "Weight-Decomposed Low-Rank Adaptation. Enhanced LoRA with better performance, slightly more compute.",
    "full": "Full",
    "fullDesc": "Full parameter fine-tuning. Trains all model weights. Requires significantly more memory.",
    "hint": "Select a method after choosing model and dataset."
  },
  "presets": {
    "title": "Quick Presets",
    "quick": "Quick Test (100 iters)",
    "standard": "Standard (1000 iters)",
    "thorough": "Thorough (2000 iters)",
    "extreme": "Extreme (5000 iters)"
  },
  "start": "Start Training",
  "stop": "Stop Training",
  "log": "Training Log",
  "noLog": "Training log will appear here once started.",
  "status": {
    "idle": "Ready to train",
    "running": "Training in progress...",
    "completed": "Training completed!",
    "failed": "Training failed"
  },
  "section": {
    "selectModel": "Select Training Model",
    "selectDataset": "Select Dataset",
    "method": "Training Method",
    "params": "Training Parameters"
  },
  "completedBanner": "Training complete! Model adapter saved.",
  "savedAt": "Saved at: ",
  "goToTest": "Go to Test Model",
  "selectModelHint": "Select a model to start training",
  "localModelPath": "Local model path",
  "invalidModelPath": "No valid MLX model files found at this path",
  "hfModelHint": "HuggingFace model (auto-downloaded during training to ~/.cache/huggingface/hub/)",
  "noDataset": "No training dataset found. Generate one in Data Preparation first.",
  "selectDatasetVersion": "Select dataset version",
  "datasetLegacy": "Legacy Dataset",
  "dataset": {
    "trainSet": "Training set",
    "validSet": "Validation set",
    "samples": "{{count}} samples",
    "sourceFiles": "Source files",
    "genType": "Type",
    "genMethod": "Method",
    "methodOllama": "AI Generation ({{model}})",
    "methodBuiltin": "Built-in Rules",
    "modeQa": "Q&A",
    "modeStyle": "Style Imitation",
    "modeChat": "Dialogue",
    "modeInstruct": "Instruction",
    "page": "Page {{current}} / {{total}}",
    "prevPage": "Previous",
    "nextPage": "Next"
  },
  "trainProgress": "Training Progress",
  "initializing": "Initializing...",
  "eta": {
    "title": "Estimated Time Left",
    "estimating": "Estimating...",
    "done": "Completed",
    "format": "~ {{time}}"
  },
  "health": {
    "title": "Training Health",
    "level": {
      "green": "Healthy",
      "yellow": "Attention Needed",
      "red": "Critical"
    },
    "hintNormal": "Training is stable. You can keep waiting.",
    "hintLossFluctuating": "Loss is slightly fluctuating, which is usually normal.",
    "hintLossRising": "Loss is trending upward. Check data quality or learning rate.",
    "hintOutOfMemory": "Memory pressure detected. Reduce batch size or enable grad checkpoint.",
    "hintError": "Training error detected. Check logs to locate the issue."
  },
  "trend": {
    "title": "Loss Trend",
    "good": "Training is converging well",
    "stable": "Loss is mostly stable, keep monitoring",
    "warning": "Loss is volatile, attention recommended"
  },
  "alerts": {
    "title": "Smart Alerts",
    "count": "{{count}} active",
    "none": "No active alerts. Training is stable.",
    "memory": {
      "title": "Memory pressure detected",
      "detail": "Training hit memory limits and may fail soon.",
      "action": "Lower batch size or max sequence length, then restart training."
    },
    "runtime": {
      "title": "Runtime exception detected",
      "detail": "An exception signature appeared in recent logs.",
      "action": "Open training logs and fix the reported script/model issue first."
    },
    "thermal": {
      "title": "Thermal throttling risk",
      "detail": "Device temperature may be limiting training speed.",
      "action": "Close heavy apps or pause briefly to cool down, then continue."
    },
    "stalled": {
      "title": "Progress appears stalled",
      "detail": "No new iterations were observed for a while.",
      "action": "Check I/O and logs. If still stuck, stop and restart with safer params."
    },
    "lossRising": {
      "title": "Loss is rising",
      "detail": "Recent train loss trend moved upward noticeably.",
      "action": "Review data quality and lower learning rate if needed."
    }
  },
  "latestTrainLoss": "Latest Train Loss:",
  "lossCurve": "Loss Curve",
  "waitingData": "Waiting for training data...",
  "lossStopped": "Training was stopped. Not enough data to draw a loss curve yet.",
  "lossNoData": "Not enough data to draw a loss curve yet.",
  "invalidModelError": "No valid model found at the selected path. Cannot start training.",
  "paramsSummary": "{{iters}} iters \u00b7 Batch {{batch}} \u00b7 {{method}}",
  "entries": "{{count}} entries",
  "step": {
    "model": "Model",
    "data": "Data",
    "method": "Method",
    "params": "Params",
    "done": "Done"
  },
  "summary": {
    "title": "Training Summary",
    "duration": "Duration",
    "finalTrainLoss": "Final Train Loss",
    "finalValLoss": "Final Val Loss",
    "totalIters": "Total Iterations",
    "baseModel": "Base Model",
    "adapterPath": "Adapter Saved To",
    "keyParams": "Key Parameters",
    "lossImprove": "Loss Improvement",
    "goToExport": "Export Model",
    "noData": "N/A",
    "report": {
      "copyMd": "Copy Report Markdown",
      "share": "System Share"
    }
  },
  "validation": {
    "needModel": "Please select a training model first",
    "needDataset": "Please select a dataset first",
    "needMethod": "Please select a training method first"
  }
}
