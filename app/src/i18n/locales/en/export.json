{
  "title": "Export Model",
  "pageTitle": "4. Export Model",
  "selectProject": "Select a project with a trained model to export",
  "noModel": "No trained model found. Complete training first.",
  "ollama": {
    "title": "Export to Ollama",
    "description": "Convert your fine-tuned LoRA adapters into a standalone Ollama model for easy local use.",
    "modelName": "Ollama Model Name",
    "modelNameHint": "Name for the model in Ollama (e.g. my-custom-model)",
    "baseModel": "Base Model",
    "baseModelHint": "Base model path or HuggingFace ID used during training",
    "adapter": "Select Adapter (fine-tuned result)",
    "adapterHint": "Select the training adapter to export, base model will be auto-filled",
    "noAdapter": "No trained adapter found. Complete training first.",
    "quantization": "Quantization",
    "q4": "4-bit (smallest, fastest)",
    "q8": "8-bit (balanced)",
    "f16": "16-bit (highest quality)",
    "exportButton": "Export to Ollama",
    "exporting": "Exporting...",
    "success": "Model exported successfully!",
    "successRunHint": "Open your terminal and run the following command:",
    "notInstalled": "Ollama is not installed. Install from ollama.com first.",
    "outputLocation": "Export output directory",
    "ollamaModelsDir": "Ollama models directory",
    "manifestDir": "Manifest directory",
    "pathDefault": "Default",
    "pathCustom": "Custom"
  },
  "ollamaPathWarning": "Configured Ollama models path '{{configuredPath}}' is missing the standard layout (manifests/registry.ollama.ai/library). Falling back to default path: {{fallbackPath}}",
  "pathWarning": "Configured export path '{{configuredPath}}' is unavailable (drive not mounted?). Files saved to local project directory: {{fallbackPath}}",
  "gguf": {
    "title": "Export as GGUF",
    "description": "Export as a .gguf file for use with llama.cpp, LM Studio, Jan, and other tools.",
    "archNote": "Supports Llama, Mistral, and Mixtral architectures only.",
    "exportButton": "Export to GGUF",
    "exporting": "Exporting...",
    "success": "GGUF exported successfully!",
    "successHint": "File saved to:",
    "openFile": "Open Folder",
    "comingSoon": "Coming in a future update",
    "pipeline": {
      "check": "Prepare",
      "resolve": "Resolve Model",
      "fuse": "Fuse & Convert"
    }
  },
  "section": {
    "selectAdapter": "Select Adapter (Training Result)",
    "modelName": "Model Name",
    "quantization": "Quantization"
  },
  "selectAdapterPlaceholder": "Select an adapter",
  "confirmDelete": "Confirm Delete",
  "deleteAdapter": "Delete adapter",
  "noWeights": "No weights",
  "deleteAll": "Delete All Adapters",
  "baseModel": "Base model: ",
  "exportLog": "Export Log",
  "pipeline": {
    "check": "Check Ollama",
    "resolve": "Resolve Model",
    "fuse": "Fuse Adapter",
    "convert": "Compatibility",
    "ollama": "Create Model"
  },
  "step": {
    "export": "Export",
    "name": "Name",
    "size": "Size"
  },
  "validation": {
    "needAdapter": "Please select an adapter first",
    "needModelName": "Please enter a model name first",
    "needQuantization": "Please select a quantization method first"
  },
  "compat": {
    "title": "Pre-export Check",
    "checking": "Checking...",
    "allGood": "All checks passed — ready to export",
    "hasIssues": "{{count}} issue(s) found — fix before exporting",
    "keys": {
      "adapter": "Adapter weights",
      "baseModel": "Base model",
      "modelName": "Model name",
      "ollama": "Ollama available"
    },
    "fix": "Fix: "
  },
  "verify": {
    "title": "Regression Verification",
    "running": "Verifying model response...",
    "success": "Model is responding normally",
    "failed": "Verification failed",
    "preview": "Response preview:",
    "retry": "Retry"
  }
}
